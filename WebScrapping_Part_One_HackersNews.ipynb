{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"website.html\", encoding=\"utf-8\") as file:\n",
    "    contents = file.read()\n",
    "soup = BeautifulSoup(contents, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>\n",
      " Angela's Personal Site\n",
      "</title>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.title.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>\n",
      " <em>\n",
      "  Founder of\n",
      "  <strong>\n",
      "   <a href=\"https://www.appbrewery.co/\">\n",
      "    The App Brewery\n",
      "   </a>\n",
      "  </strong>\n",
      "  .\n",
      " </em>\n",
      "</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.p.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Angela's Personal Site\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 id=\"name\">\n",
      "   Angela Yu\n",
      "  </h1>\n",
      "  <p>\n",
      "   <em>\n",
      "    Founder of\n",
      "    <strong>\n",
      "     <a href=\"https://www.appbrewery.co/\">\n",
      "      The App Brewery\n",
      "     </a>\n",
      "    </strong>\n",
      "    .\n",
      "   </em>\n",
      "  </p>\n",
      "  <p>\n",
      "   I am an iOS and Web Developer. I ❤️ coffee and motorcycles.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <h3 class=\"heading\">\n",
      "   Books and Teaching\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    The Complete iOS App Development Bootcamp\n",
      "   </li>\n",
      "   <li>\n",
      "    The Complete Web Development Bootcamp\n",
      "   </li>\n",
      "   <li>\n",
      "    100 Days of Code - The Complete Python Bootcamp\n",
      "   </li>\n",
      "  </ul>\n",
      "  <hr/>\n",
      "  <h3 class=\"heading\">\n",
      "   Other Pages\n",
      "  </h3>\n",
      "  <a href=\"https://angelabauer.github.io/cv/hobbies.html\">\n",
      "   My Hobbies\n",
      "  </a>\n",
      "  <a href=\"https://angelabauer.github.io/cv/contact-me.html\">\n",
      "   Contact Me\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()) # prettify() method helps to beutify the html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://www.appbrewery.co/\">The App Brewery</a>, <a href=\"https://angelabauer.github.io/cv/hobbies.html\">My Hobbies</a>, <a href=\"https://angelabauer.github.io/cv/contact-me.html\">Contact Me</a>]\n"
     ]
    }
   ],
   "source": [
    "all_anchor_tags = soup.find_all(name='a') # finding all tags with find_all() function where the tag name is a / li / tr \n",
    "#name arguemnt is the name of tags\n",
    "print(all_anchor_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The App Brewery\n",
      "My Hobbies\n",
      "Contact Me\n"
     ]
    }
   ],
   "source": [
    "for tag in all_anchor_tags:\n",
    "    print(tag.getText()) # looping throughyt the txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.appbrewery.co/\n",
      "https://angelabauer.github.io/cv/hobbies.html\n",
      "https://angelabauer.github.io/cv/contact-me.html\n"
     ]
    }
   ],
   "source": [
    "for tag in all_anchor_tags:\n",
    "    print(tag.get(\"href\")) # looping throughyt the taGS and getting the value of \"href\" attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 id=\"name\">Angela Yu</h1>]\n"
     ]
    }
   ],
   "source": [
    "heading = soup.find_all(name='h1', id=\"name\") # finding first tag with find() function where the tag name is h1\n",
    "#name arguemnt is the name of tags and the attribute name is \"id\" whose value is \"name\"\n",
    "#if you use find_all it will look for only the given parameters and match for retrieval \n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heading']\n"
     ]
    }
   ],
   "source": [
    "heading = soup.find(name='h3', class_=\"heading\") # finding first tag with find() function where the tag name is h3\n",
    "#name arguemnt is the name of tags and the attribute name is \"class\" whose value is \"heading\" \n",
    "# important NOTE : as \"CLASS\" a keyword in python, this is why we need to use _ so that the python knows that this is not the python \n",
    "# key\n",
    "#if you use find_all it will look for only the given parameters and match for retrieval \n",
    "\n",
    "\n",
    "#print(heading.getText())\n",
    "print(heading.get(\"class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://www.appbrewery.co/\">The App Brewery</a>\n"
     ]
    }
   ],
   "source": [
    "link = soup.select_one(\"p a\") # \"p a\" means a tag under p tag\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"name\">Angela Yu</h1>\n"
     ]
    }
   ],
   "source": [
    "name = soup.select_one(\"#name\") # use # before the value \"name\" of id tag\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"heading\">Books and Teaching</h3>\n"
     ]
    }
   ],
   "source": [
    "heading = soup.select_one(\".heading\") # use . before the value \"heading\" of class tag\n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be using live website to scrap data from it \n",
    "### he website is : https://news.ycombinator.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://news.ycombinator.com/news\")\n",
    "hackersnews = response.text\n",
    "# Now that we got hold of the web page in hackersnews variable\n",
    "# we will use beutifulshoup to levarge the automation\n",
    "soup = BeautifulSoup(hackersnews, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starlink lost 40 satellites to a geomagnetic storm\n",
      "Heuristics that almost always work\n",
      "Atari asks for help finding developer of 2600 game Aquaventure\n",
      "The Kelly criterion: How to size bets (2019)\n",
      "Why is the Zoom app listening on my microphone when not in a meeting?\n",
      "Apple unveils contactless payments via Tap to Pay on iPhone\n",
      "A Lonely Universe\n",
      "Raid-Z Expansion Feature for ZFS Goes Live\n",
      "Texas power crisis revealed flaw in market’s design\n",
      "Gyroflow: Advanced open-source gyro-assisted video stabilization\n",
      "Nerdle = Like Wordle but with Equations\n",
      "Number of Earth’s tree species estimated to be 14% higher than currently known\n",
      "Christopher Walken on Acting Like Christopher Walken\n",
      "Phenethylamines I have feared and loathed (2020)\n",
      "What driveling times are these! Georgian grumblings on an age of decline\n",
      "Netbeez (YC W16) is hiring a React.js engineer – remote (US), contract to hire\n",
      "Feds arrest couple, seize $3.6B in hacked Bitcoin funds\n",
      "PDEs you should know\n",
      "The Semiconductor Ecosystem Explained\n",
      "Exposing a web service with Cloudflare Tunnel\n",
      "How UTF-8 Works\n",
      "Django: Reformatted code with Black\n",
      "Tesla FSD Beta Lunges Toward Bicyclist\n",
      "My Year of Reading Lemmishly\n",
      "The Problem with Frameworks\n",
      "The Common Tongue of Twenty-First-Century London\n",
      "Surfing with the Linker Aliens: Solaris Linking and ELF Blogs\n",
      "An ancient geometry problem falls to new mathematical techniques\n",
      "Show HN: Three Magic Words\n",
      "Common Calendrical Fallacies\n",
      "==================================\n",
      "https://www.spacex.com/updates/\n",
      "https://astralcodexten.substack.com/p/heuristics-that-almost-always-work\n",
      "https://venturebeat.com/2022/02/08/atari-asks-gamers-for-help-finding-developer-of-mysterious-2600-game/\n",
      "https://explore.paulbutler.org/bet/\n",
      "https://community.zoom.com/t5/Meetings/Why-is-the-Zoom-app-listening-on-my-microphone-when-not-in-a/td-p/29019\n",
      "https://www.apple.com/newsroom/2022/02/apple-unveils-contactless-payments-via-tap-to-pay-on-iphone/\n",
      "https://inference-review.com/article/a-lonely-universe\n",
      "https://freebsdfoundation.org/blog/raid-z-expansion-feature-for-zfs-goes-live/\n",
      "https://news.cornell.edu/stories/2022/02/texas-power-crisis-revealed-flaw-markets-design\n",
      "https://gyroflow.xyz/\n",
      "https://nerdlegame.com/?a=0\n",
      "https://news.umich.edu/number-of-earths-tree-species-estimated-to-be-14-higher-than-currently-known-with-some-9200-species-yet-to-be-discovered/\n",
      "https://www.nytimes.com/interactive/2022/02/07/magazine/christopher-walken-interview.html\n",
      "http://nikobidin.com/phenethylamines-i-have-feared-and-loathed\n",
      "https://www.laphamsquarterly.org/roundtable/what-driveling-times-are-these\n",
      "https://netbeez.bamboohr.com/jobs/view.php?id=21&source=aWQ9MjM%3D\n",
      "https://www.washingtonpost.com/national-security/2022/02/08/bitfinex-hack-bitcoin-arrests/\n",
      "https://www.lucaspauker.com/pdes\n",
      "https://semiwiki.com/semiconductor-manufacturers/307494-the-semiconductor-ecosystem-explained/\n",
      "https://erisa.dev/exposing-a-web-service-with-cloudflare-tunnel/\n",
      "https://sethmlarson.dev/blog/utf-8\n",
      "https://github.com/django/django/pull/15387\n",
      "https://twitter.com/omedyentral/status/1491146820097298433\n",
      "https://www.lrb.co.uk/the-paper/v44/n03/jonathan-lethem/my-year-of-reading-lemmishly\n",
      "https://plbrault.com/blog-posts/the-problem-with-frameworks-en/\n",
      "https://www.newyorker.com/culture/personal-history/the-common-tongue-of-twenty-first-century-london\n",
      "http://www.linker-aliens.org/\n",
      "https://www.quantamagazine.org/an-ancient-geometry-problem-falls-to-new-mathematical-techniques-20220208/\n",
      "https://www.threemagicwords.app/play/\n",
      "https://yourcalendricalfallacyis.com/\n",
      "==================================\n",
      "34 points\n",
      "549 points\n",
      "170 points\n",
      "81 points\n",
      "93 points\n",
      "753 points\n",
      "33 points\n",
      "122 points\n",
      "77 points\n",
      "292 points\n",
      "244 points\n",
      "62 points\n",
      "56 points\n",
      "100 points\n",
      "28 points\n",
      "485 points\n",
      "88 points\n",
      "72 points\n",
      "336 points\n",
      "251 points\n",
      "247 points\n",
      "173 points\n",
      "14 points\n",
      "52 points\n",
      "20 points\n",
      "7 points\n",
      "153 points\n",
      "183 points\n",
      "42 points\n"
     ]
    }
   ],
   "source": [
    "Anchortags = soup.find_all(name='a',class_=\"titlelink\")\n",
    "for links in Anchortags:\n",
    "    print(links.getText())\n",
    "print(\"==================================\")\n",
    "for links in Anchortags:\n",
    "    print(links.get(\"href\"))\n",
    "print(\"==================================\")\n",
    "score = soup.find_all(name='span', class_=\"score\")\n",
    "for vote in score:\n",
    "    print(vote.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starlink lost 40 satellites to a geomagnetic storm\n",
      "https://www.spacex.com/updates/\n",
      "34 points\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "response = requests.get(\"https://news.ycombinator.com/news\") \n",
    "yc_web_page = response.text \n",
    "soup = BeautifulSoup(yc_web_page, \"html.parser\") \n",
    "article_tag = soup.find(name=\"a\", class_=\"titlelink\") # href can change any time\n",
    "article_text = article_tag.getText()\n",
    "article_link = article_tag.get(\"href\") \n",
    "article_upvote = soup.find(name=\"span\", class_ =\"score\").getText() \n",
    "#this is the simple way of thinking.\n",
    "#after geting single entity at a time now we can think of getting all of the items \n",
    "print(article_text) \n",
    "print(article_link) \n",
    "print(article_upvote) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "articale = soup.find_all(name=\"a\",class_=\"titlelink\")\n",
    "articale_texts = []\n",
    "article_links = []\n",
    "article_votes =[]\n",
    "for artical_tag in articale:\n",
    "    articale_text = artical_tag.getText()\n",
    "    articale_texts.append(articale_text)\n",
    "    article_link = artical_tag.get(\"href\")\n",
    "    article_links.append(article_link)\n",
    "    article_vote =  soup.find_all(name=\"span\",class_=\"score\")\n",
    "    for vote in article_vote:\n",
    "        votes = vote.getText().split()[0]\n",
    "        article_votes.append(int(votes))\n",
    "        \n",
    "    # short of this for loop\n",
    "    #article_vote = [ vote.getText() for vote in soup.find_all(name=\"span\",class_=\"score\") ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.apple.com/newsroom/2022/02/apple-unveils-contactless-payments-via-tap-to-pay-on-iphone/\n",
      "Apple unveils contactless payments via Tap to Pay on iPhone\n",
      "heighst vote 753\n"
     ]
    }
   ],
   "source": [
    "#print(articale)\n",
    "#print(articale_texts)\n",
    "#print(article_links)\n",
    "#print(article_votes)\n",
    "####\n",
    "#Our target to get the heighest upvoted article on hackers news \n",
    "####\n",
    "highest = max(article_votes)\n",
    "index = article_votes.index(highest)\n",
    "print(article_links[index])\n",
    "print(articale_texts[index])\n",
    "print(f\"heighst vote {highest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
